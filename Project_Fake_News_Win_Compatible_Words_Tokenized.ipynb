{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Group members: \n",
    "- Marcus Thorn Mathiesen - zkj257 \n",
    "- Christian Arboe Franck - fpk278\n",
    "\n",
    "Group number: 4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports used for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import csv\n",
    "import datetime\n",
    "from cleantext import clean\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2 import connect, extensions, sql, Error\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Load database\n",
    "\n",
    "- Demonstrate that you have a working database containing the FakeNewsCorpus dataset.\n",
    "\n",
    "- Explain your choice of schema design. \n",
    "\n",
    "- Demonstrate that your database contains a larger number of rows (e.g. one million - or however many you can reasonably work with on your available hardware)\n",
    "\n",
    "- Demonstrate that your database supports simple queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create csv files that contains the data from the raw data csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "csv_in = '1mio-raw.csv' #You need to have the file in the same folder as this Jupyter Notebook"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Info about the fields:\n",
    "\n",
    "'Unnamed: 0'       - The sample's index in the csv file (type: number) It's zero indexed \n",
    "'id'               - The identification number given to each article (type: number)       \n",
    "'domain'           - The domain of the sample's url (type: string)\n",
    "'type'             - The reliability of the source (type: string/enum) \n",
    "'url'              - (type: string)\n",
    "'content'          - The actual sample (type: string with quotation marks around it) \n",
    "\n",
    "'scraped_at'       - The date at which the sample was scraped from the internet (type: datetime)\n",
    "'inserted_at'      - The date at which the sample was inserted into the csv file or a data base (type: datetime)\n",
    "'updated_at'       - The date at which the sample was updated in the csv file or a data base (type: datetime)  \n",
    "\n",
    "'title'            - (type: string)\n",
    "'authors'          - (type: string where authors are seperated by commas)\n",
    "'keywords'         - Always empty \n",
    "'meta_keywords'    - Actual list of keywords (type: list of strings)\n",
    "'meta_description' - Some small text related to the sample (type: string)\n",
    "'tags'             - (type: string where different tags are seperated by commas)\n",
    "'summary'          - Always empty\n",
    "'source'           - The source of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dictionaries\n",
    "author_dictionary  = dict()\n",
    "domain_dictionary  = dict() \n",
    "type_dictionary    = dict()\n",
    "keyword_dictionary = dict()   \n",
    "article_dictionary = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_empty_file_for_writing(filename):\n",
    "    if os.path.exists(filename):\n",
    "      os.remove(filename)\n",
    "    return open(filename, \"w\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_author     = create_empty_file_for_writing('author.csv')\n",
    "csv_keyword    = create_empty_file_for_writing('keyword.csv')\n",
    "csv_domain     = create_empty_file_for_writing('domain.csv')\n",
    "csv_type       = create_empty_file_for_writing('type.csv')\n",
    "csv_article    = create_empty_file_for_writing('article.csv')\n",
    "csv_written_by = create_empty_file_for_writing('written_by.csv')\n",
    "csv_tags       = create_empty_file_for_writing('tags.csv')\n",
    "csv_webpage    = create_empty_file_for_writing('webpage.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_in_dictionary(dictionary, liste, ID):\n",
    "    dictionary.clear()\n",
    "    liste = list(set(liste)) # By converting the list to a set we sort the list and remove duplicates\n",
    "    for j in range(len(liste)):\n",
    "        info = str(liste[j])\n",
    "        dictionary[info] = ID\n",
    "        ID += 1\n",
    "    return ID    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_entity_to_CSV(filename, idName, keyName, dictionary):\n",
    "    file = open(filename,\"a\",encoding='utf-8')\n",
    "    for item in dictionary.items():\n",
    "        file.write(\"%s,%s\\n\" %(str(item[1]), str(item[0])))\n",
    "    file.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "def clean_text(content):\n",
    "\n",
    "    # split into words\n",
    "    tokens = word_tokenize(content)\n",
    "    \n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    clean_text = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    #Join to one string\n",
    "    seperator = ' '\n",
    "    clean_text = seperator.join(clean_text)\n",
    "    \"\"\"\n",
    "    # Set all words to be lowercased\n",
    "    clean_text = content.lower()\n",
    "    \n",
    "    # Clean dates \n",
    "    date1 = r\"\\b(?:jan(?:uary|\\.)?|feb(?:ruary|\\.)?|mar(?:ch|\\.)?|apr(?:il|\\.)?|may|jun(?:e|\\.)?|jul(?:y|\\.)?|aug(?:ust|\\.)?|sep(?:tember|\\.)?|oct(?:ober|\\.)?|(nov|dec)(?:ember|\\.)?) (?:\\d{1,2})(,)? (?:1\\d{3}|2\\d{3})(?=\\D|$)\"\n",
    "    date2 = r\"\\b(\\d{1, 2})? (?:jan(?:uary|\\.)?|feb(?:ruary|\\.)?|mar(?:ch|\\.)?|apr(?:il|\\.)?|may|jun(?:e|\\.)?|jul(?:y|\\.)?|aug(?:ust|\\.)?|sep(?:tember|\\.)?|oct(?:ober|\\.)?|(nov|dec)(?:ember|\\.)?) (?:1\\d{3}|2\\d{3})(?=\\D|$)\"\n",
    "    date3 = r\"\\b(?:jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|jul(?:y)?|aug(?:ust)?|sep(?:tember)?|oct(?:ober)?|(nov|dec)(?:ember)?) (of)? (?:(?:\\d{1,2})|(?:1\\d{3}|2\\d{3}))(?=\\D|$)\"                         \n",
    "    date4 = r\"(?:\\d{1,2}/\\d{1,2}/\\d{4}|\\d{1,2}-\\d{1,2}-\\d{4})\"\n",
    "    date_patterns = [date1, date2, date3, date4]\n",
    "    \n",
    "    for pattern in date_patterns:\n",
    "        clean_text = re.sub(pattern, ' <DATE> ', clean_text)\n",
    "    \n",
    "    # Clean email\n",
    "    email1 = r'([\\w0-9._-]+@[\\w0-9._-]+\\.[\\w0-9_-]+)'\n",
    "    clean_text = re.sub(email1, ' <EMAIL> ', clean_text)\n",
    "    \n",
    "    # Clean URLs \n",
    "    url1 = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    clean_text = re.sub(url1, ' <URL> ', clean_text)\n",
    "    \n",
    "    # Clean numbers\n",
    "    num1 = r'[0-9]+'\n",
    "    clean_text = re.sub(num1, ' <NUM> ', clean_text)\n",
    "    \n",
    "    # Clean multiple white spaces, tabs, and newlines\n",
    "    space1 = r\"\\s+\"\n",
    "    clean_text = re.sub(space1, ' ', clean_text)\n",
    "    clean_text = clean_text.replace('\\n', '')\n",
    "    \n",
    "    # Remove ^ since we use them as delimiter\n",
    "    clean_text = clean_text.replace('^', '')\n",
    "    \n",
    "    # Remove \"\n",
    "    clean_text = clean_text.replace('\"', '')\n",
    "    \n",
    "    # Remove { since we use them as delimiter\n",
    "    clean_text = clean_text.replace('{', '')\n",
    "    clean_text = clean_text.replace('}', '')\n",
    "    \"\"\"\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNaN(string):\n",
    "    return string != string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_author = ID_domain = ID_keyword = ID_type = ID_article = 0\n",
    "def extract_and_put_in_csv_files(chunk):\n",
    "    \n",
    "    temp1 = []\n",
    "    temp2 = []\n",
    "    temp3 = []\n",
    "    temp4 = []\n",
    "    \n",
    "    global ID_author\n",
    "    global ID_domain\n",
    "    global ID_keyword\n",
    "    global ID_type \n",
    "    global ID_article\n",
    "    \n",
    "    \n",
    "    for index, row in chunk.iterrows():\n",
    "        # Authors  \n",
    "        authors = row['authors']\n",
    "        if(not isNaN(authors)):\n",
    "            list_of_authors = authors.split(\", \")\n",
    "            for author in list_of_authors:\n",
    "                if(len(author) <= 64):\n",
    "                    temp1.append(author)    \n",
    "        \n",
    "        # Meta keywords\n",
    "        keywords = row['meta_keywords']\n",
    "        if(not isNaN(keywords)):\n",
    "            list_of_keywords = re.split(r'[;,\"\\'\\[\\]]\\s*', keywords)\n",
    "            for keyword in list_of_keywords:\n",
    "                if(len(keyword) <= 128):\n",
    "                    temp2.append(keyword)\n",
    "        \n",
    "        # Domain\n",
    "        domain = row['domain']\n",
    "        if(not isNaN(domain)):\n",
    "            if(len(domain) <= 1024):\n",
    "                temp3.append(domain)\n",
    "        \n",
    "        # Type\n",
    "        typ = row['type']\n",
    "        if(not isNaN(typ)):\n",
    "            if(len(typ) <= 64):\n",
    "                temp4.append(typ)\n",
    "        \n",
    "    # Authors    \n",
    "    ID_author = put_in_dictionary(author_dictionary, temp1, ID_author)\n",
    "    #with open('author.csv', 'w', newline='') as authorfile:\n",
    "    #    fieldnames = ['author_id', 'author_name']\n",
    "    #    writer = csv.DictWriter(authorfile, fieldnames=fieldnames)\n",
    "    #authorfile.close()\n",
    "    simple_entity_to_CSV(\"author.csv\", \"author_id\", \"author_name\", author_dictionary)\n",
    "    \n",
    "    # Meta keywords\n",
    "    ID_domain = put_in_dictionary(keyword_dictionary, temp2, ID_domain)\n",
    "    simple_entity_to_CSV(\"keyword.csv\", \"keyword_id\", \"keyword\", keyword_dictionary)\n",
    "    \n",
    "    # Domain\n",
    "    ID_keyword = put_in_dictionary(domain_dictionary, temp3, ID_keyword)\n",
    "    simple_entity_to_CSV(\"domain.csv\", \"domain_id\", \"domain_url\", domain_dictionary)\n",
    "    \n",
    "    # Type\n",
    "    ID_type = put_in_dictionary(type_dictionary, temp4, ID_type)\n",
    "    simple_entity_to_CSV(\"type.csv\", \"type_id\", \"type_name\", type_dictionary)\n",
    "        \n",
    "\n",
    "        \n",
    "    for index, row in chunk.iterrows():\n",
    "        # Article\n",
    "        title = row['title']\n",
    "        if(title and (not isNaN(title)) and (len(title) <= 512)):\n",
    "            title = clean_text(title)\n",
    "        else:\n",
    "            title = \"NULL\"\n",
    "        \n",
    "        content = row['content']\n",
    "        if(not isNaN(content)):\n",
    "            content = clean_text(content)\n",
    "        else:\n",
    "            content = \"NULL\"\n",
    "        \n",
    "        summary = row['summary']\n",
    "        if(summary and (not isNaN(summary))):\n",
    "            summary = clean_text(summary)\n",
    "        else:\n",
    "            summary = \"NULL\"\n",
    "        \n",
    "        meta_description = row['meta_description']\n",
    "        if(meta_description and (not isNaN(meta_description))):\n",
    "            meta_description = clean_text(meta_description)\n",
    "        else:\n",
    "            meta_description = \"NULL\"\n",
    "        \n",
    "        typ = row['type']\n",
    "        if((not isNaN(typ)) and (len(typ) <= 64)):\n",
    "            type_id = type_dictionary[typ]\n",
    "        else:\n",
    "            type_id = 0\n",
    "        \n",
    "        scrapped_at = row['scraped_at']\n",
    "        if(isNaN(scrapped_at)):\n",
    "            scrapped_at = datetime.datetime(1000, 1, 1)\n",
    "        inserted_at = row['inserted_at']\n",
    "        if(isNaN(inserted_at)):\n",
    "            inserted_at = datetime.datetime(1000, 1, 1)\n",
    "        updated_at  = row['updated_at']\n",
    "        if(isNaN(updated_at)):\n",
    "            updated_at = datetime.datetime(1000, 1, 1)\n",
    "        \n",
    "        #\n",
    "        #ID_author = put_in_dictionary(author_dictionary, )\n",
    "        \n",
    "        #print(f\"^%s^%s^%s\\n\" % (title, content, meta_description))\n",
    "        #csv_article = open(\"article.csv\",\"a\",encoding='utf-8')\n",
    "        csv_article.write(\"%s^\\\"%s\\\"^\\\"%s\\\"^\\\"%s\\\"^\\\"%s\\\"^%s^%s^%s^%s\\n\"%\n",
    "            (ID_article\n",
    "             , title.encode(encoding='utf_8', errors='replace')#.decode(encoding='utf_8', errors='ignore')\n",
    "             , content.encode(encoding='utf_8', errors='replace')#.decode(encoding='utf_8', errors='ignore')\n",
    "             , summary.encode(encoding='utf_8', errors='replace')#.decode(encoding='utf_8', errors='ignore')\n",
    "             , meta_description.encode(encoding='utf_8', errors='replace')#.decode(encoding='utf_8', errors='ignore')\n",
    "             , type_id\n",
    "             , scrapped_at\n",
    "             , inserted_at\n",
    "             , updated_at))\n",
    "        #csv_article.close()\n",
    "        \n",
    "        # Webpage\n",
    "        url = row['url']\n",
    "        domain = row['domain']\n",
    "        if((not isNaN(url)) and (not isNaN(domain)) and (len(url) <= 1024) and (len(domain) <= 1024)):\n",
    "            #csv_webpage = open(\"webpage.csv\",\"a\",encoding='utf-8')\n",
    "            csv_webpage.write(\"%s^%s^%s\\n\" % (ID_article, url, domain_dictionary[domain]))\n",
    "            #csv_webpage.close()\n",
    "        \n",
    "        # Writen_by\n",
    "        authors = row['authors']\n",
    "        if(not isNaN(authors)):\n",
    "            list_of_authors = authors.split(\", \")\n",
    "            for author in list_of_authors:\n",
    "                if(len(author) <= 64):\n",
    "                    #csv_written_by = open(\"written_by.csv\",\"a\",encoding='utf-8')\n",
    "                    csv_written_by.write(\"%s,%s\\n\" % (ID_article, author_dictionary[author]))\n",
    "                    #csv_written_by.close()\n",
    "        \n",
    "        # Tags\n",
    "        keywords = row['meta_keywords']\n",
    "        if(not isNaN(keywords)):\n",
    "            keywords = re.split(r'[;,\"\\'\\[\\]]\\s*', keywords)\n",
    "            for keyword in keywords:\n",
    "                if(len(keyword) <= 128):\n",
    "                    #csv_tags = open(\"tags.csv\",\"a\",encoding='utf-8')\n",
    "                    csv_tags.write(\"%s,%s\\n\" % (ID_article, keyword_dictionary[keyword]))\n",
    "                    #csv_tags.close()\n",
    "        \n",
    "        ID_article += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-81e246839c2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mchunk_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m# Used to see how far the reading process is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mextract_and_put_in_csv_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCHUNK_SIZE\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mchunk_idx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" rows have been read.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mchunk_idx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-9b53df3b1925>\u001b[0m in \u001b[0;36mextract_and_put_in_csv_files\u001b[1;34m(chunk)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0misNaN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"NULL\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-fb6dfc8e9ad0>\u001b[0m in \u001b[0;36mclean_text\u001b[1;34m(content)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# split into words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# convert to lower case\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     return [\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m     ]\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     return [\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m     ]\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_parentheses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_str\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTARTING_QUOTES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "csv_author.close()     \n",
    "csv_keyword.close()\n",
    "csv_domain.close()\n",
    "csv_type.close()       \n",
    "csv_article.close()    \n",
    "csv_written_by.close() \n",
    "csv_tags.close()       \n",
    "csv_webpage.close()\n",
    "csv_author     = create_empty_file_for_writing('author.csv')\n",
    "csv_keyword    = create_empty_file_for_writing('keyword.csv')\n",
    "csv_domain     = create_empty_file_for_writing('domain.csv')\n",
    "csv_type       = create_empty_file_for_writing('type.csv')\n",
    "csv_article    = create_empty_file_for_writing('article.csv')\n",
    "csv_written_by = create_empty_file_for_writing('written_by.csv')\n",
    "csv_tags       = create_empty_file_for_writing('tags.csv')\n",
    "csv_webpage    = create_empty_file_for_writing('webpage.csv')\n",
    "# Process and store the stuff \n",
    "CHUNK_SIZE = 100000 # read 100k rows at a time\n",
    "\n",
    "with open(csv_in, 'r', newline='',encoding='utf-8', errors='replace') as csvfile:\n",
    "    reader = pd.read_csv(csvfile, chunksize=CHUNK_SIZE, encoding='utf-8',parse_dates=['scraped_at','inserted_at','updated_at'])\n",
    "    \"\"\", dtype={\"Unnamed: 0\": int64, \"id\": int64, \"domain\": object,\n",
    "                                                              \"type\": object,\"url\": object,\"content\": object,\n",
    "                                                              \"scraped_at\": object,\"inserted_at\": object,\"updated_at\": object,\n",
    "                                                              \"title\": object, \"authors\": object,\"meta_keywords\": object,\n",
    "                                                              \"meta_description\": object,\"tags\": object ,\"summary\": object,\n",
    "                                                              \"source\": object}\"\"\"\n",
    "    chunk_idx = 1 # Used to see how far the reading process is\n",
    "    for chunk in reader:\n",
    "        extract_and_put_in_csv_files(chunk)\n",
    "        print(str(CHUNK_SIZE * chunk_idx) + \" rows have been read.\")\n",
    "        chunk_idx += 1\n",
    "csvfile.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create database and load it with the data from above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the follwoing commands in the terminal to create the database:\n",
    "\n",
    "sudo -u postgres psql\n",
    "\n",
    "CREATE DATABASE fakenews;\n",
    "\n",
    "CREATE USER user123 WITH ENCRYPTED PASSWORD 'pass123';\n",
    "\n",
    "GRANT ALL PRIVILEGES ON DATABASE FakeNews TO user123;\n",
    "\n",
    "GRANT pg_read_server_files TO user123;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query_create_and_load_database = \"\"\"\n",
    "DROP TABLE IF EXISTS Written_by, Tags, Webpage, Article, Typ, Domain, Author, Keyword CASCADE;\n",
    "\n",
    "\n",
    "CREATE TABLE Keyword (\n",
    "  keyword_id serial,\n",
    "  keyword varchar(128),\n",
    "  PRIMARY KEY (keyword_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE Author (\n",
    "  author_id serial,\n",
    "  author_name varchar(64),\n",
    "  PRIMARY KEY (author_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE Domain (\n",
    "  domain_id serial,\n",
    "  domain_url varchar(1024),\n",
    "  PRIMARY KEY (domain_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE Typ (\n",
    "  type_id serial,\n",
    "  type_name varchar(64),\n",
    "  PRIMARY KEY (type_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE Article (\n",
    "  article_id integer,\n",
    "  title text,\n",
    "  content text,\n",
    "  summary text,\n",
    "  meta_description text,\n",
    "  type_id integer REFERENCES Typ(type_id),\n",
    "  inserted_at timestamp,\n",
    "  updated_at timestamp,\n",
    "  scraped_at timestamp,\n",
    "  PRIMARY KEY (article_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE Webpage (\n",
    "  article_id integer REFERENCES Article(article_id),\n",
    "  url varchar(1024),\n",
    "  domain_id integer REFERENCES Domain(domain_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE Tags (\n",
    "  article_id integer REFERENCES Article(article_id),\n",
    "  keyword_id integer REFERENCES Keyword(keyword_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE Written_by (\n",
    "  article_id integer REFERENCES Article(article_id),\n",
    "  author_id integer REFERENCES Author(author_id)\n",
    ");\n",
    "\n",
    "COPY keyword    FROM '/Users/Public/Project_Data_Science/keyword.csv'     DELIMITER ',' CSV;\n",
    "COPY author     FROM '/Users/Public/Project_Data_Science/author.csv'      DELIMITER ',' CSV;\n",
    "COPY typ        FROM '/Users/Public/Project_Data_Science/type.csv'        DELIMITER ',' CSV;\n",
    "COPY domain     FROM '/Users/Public/Project_Data_Science/domain.csv'      DELIMITER ',' CSV;\n",
    "COPY article    FROM '/Users/Public/Project_Data_Science/article.csv'     DELIMITER '^' QUOTE '}' CSV ;\n",
    "COPY tags       FROM '/Users/Public/Project_Data_Science/tags.csv'      DELIMITER ',' CSV;\n",
    "COPY written_by FROM '/Users/Public/Project_Data_Science/written_by.csv' DELIMITER ',' CSV;\n",
    "COPY webpage    FROM '/Users/Public/Project_Data_Science/webpage.csv'   DELIMITER '^' CSV;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Establishing the connection\n",
    "conn = psycopg2.connect(\n",
    "   database=\"fakenews\", user='user123', password='pass123', host='127.0.0.1', port= '5432'\n",
    ")\n",
    "\n",
    "# Set the isolation level\n",
    "conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "\n",
    "# Creating a cursor object using the cursor() method\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute SQL commands\n",
    "cursor.execute(sql_query_create_and_load_database)\n",
    "\n",
    "# Close the cursor to avoid memory leaks\n",
    "cursor.close()\n",
    "\n",
    "# Close the connection to avoid memory leaks\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to the database to see that the data has been loaded with the following command:\n",
    "\n",
    "psql -h localhost -U user123 fakenews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the database design \n",
    "\n",
    "We have chosen to design our database this way since ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration that our database contains a larger number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query_large_number_of_rows_keyword    = \"SELECT COUNT(*) FROM keyword;\"\n",
    "sql_query_large_number_of_rows_author     = \"SELECT COUNT(*) FROM author;\"\n",
    "sql_query_large_number_of_rows_domain     = \"SELECT COUNT(*) FROM domain;\"\n",
    "sql_query_large_number_of_rows_typ        = \"SELECT COUNT(*) FROM typ;\"\n",
    "sql_query_large_number_of_rows_article    = \"SELECT COUNT(*) FROM article;\"\n",
    "sql_query_large_number_of_rows_webpage    = \"SELECT COUNT(*) FROM webpage;\"\n",
    "sql_query_large_number_of_rows_tags       = \"SELECT COUNT(*) FROM tags;\"\n",
    "sql_query_large_number_of_rows_written_by = \"SELECT COUNT(*) FROM written_by;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the keyword table: 33321\n",
      "Number of rows in the author table: 10376\n",
      "Number of rows in the domain table: 246\n",
      "Number of rows in the typ table: 12\n",
      "Number of rows in the article table: 21790\n",
      "Number of rows in the webpage table: 21767\n",
      "Number of rows in the tags table: 162193\n",
      "Number of rows in the written_by table: 29723\n"
     ]
    }
   ],
   "source": [
    "# Establishing the connection\n",
    "conn = psycopg2.connect(\n",
    "   database=\"fakenews\", user='user123', password='pass123', host='127.0.0.1', port= '5432'\n",
    ")\n",
    "\n",
    "# Set the isolation level\n",
    "conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "\n",
    "# Creating a cursor object using the cursor() method\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute SQL command\n",
    "cursor.execute(sql_query_large_number_of_rows_keyword)\n",
    "# Get result from SQL command\n",
    "print(\"Number of rows in the keyword table: \" + str(cursor.fetchone()[0]))\n",
    "\n",
    "# Execute SQL command\n",
    "cursor.execute(sql_query_large_number_of_rows_author)\n",
    "# Get result from SQL command\n",
    "print(\"Number of rows in the author table: \" + str(cursor.fetchone()[0]))\n",
    "\n",
    "# Execute SQL command\n",
    "cursor.execute(sql_query_large_number_of_rows_domain)\n",
    "# Get result from SQL command\n",
    "print(\"Number of rows in the domain table: \" + str(cursor.fetchone()[0]))\n",
    "\n",
    "# Execute SQL command\n",
    "cursor.execute(sql_query_large_number_of_rows_typ)\n",
    "# Get result from SQL command\n",
    "print(\"Number of rows in the typ table: \" + str(cursor.fetchone()[0]))\n",
    "\n",
    "# Execute SQL command\n",
    "cursor.execute(sql_query_large_number_of_rows_article)\n",
    "# Get result from SQL command\n",
    "print(\"Number of rows in the article table: \" + str(cursor.fetchone()[0]))\n",
    "\n",
    "# Execute SQL command\n",
    "cursor.execute(sql_query_large_number_of_rows_webpage)\n",
    "# Get result from SQL command\n",
    "print(\"Number of rows in the webpage table: \" + str(cursor.fetchone()[0]))\n",
    "\n",
    "# Execute SQL command\n",
    "cursor.execute(sql_query_large_number_of_rows_tags)\n",
    "# Get result from SQL command\n",
    "print(\"Number of rows in the tags table: \" + str(cursor.fetchone()[0]))\n",
    "\n",
    "# Execute SQL command\n",
    "cursor.execute(sql_query_large_number_of_rows_written_by)\n",
    "# Get result from SQL command\n",
    "print(\"Number of rows in the written_by table: \" + str(cursor.fetchone()[0]))\n",
    "\n",
    "\n",
    "# Close the cursor to avoid memory leaks\n",
    "cursor.close()\n",
    "\n",
    "# Close the connection to avoid memory leaks\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Issuing queries to better understand the characteristics of the data\n",
    "\n",
    "\n",
    "Formulate the following queries in the database languages requested (in the square brackets following each item) and briefly discuss what you observe when you execute them over your database: \n",
    "\n",
    "3.1: List the domains of news articles of reliable type and scrapped at or after January 15, 2018. NOTE: Do not include duplicate domains in your answer. [Languages: relational algebra and SQL]\n",
    "\n",
    "3.2: List the name(s) of the most prolific author(s) of news articles of fake type. An author is among the most prolific if it has authored as many or more fake news articles as any other author in the dataset. [Languages: extended relational algebra and SQL]\n",
    "\n",
    "3.3: Count the pairs of article IDs that exhibit the exact same set of meta-keywords, but only return the pairs where the set of meta-keywords is not empty. [Language: SQL]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3.1\n",
    "\n",
    "SQL\n",
    "\n",
    "SELECT distinct domain_url \n",
    "FROM Domain NATURAL JOIN webpage NATURAL JOIN article NATURAL JOIN typ \n",
    "WHERE type_name = 'reliable';\n",
    "\n",
    "\n",
    "\n",
    "Relational Algebra\n",
    "\n",
    "π_{domain_url} σ_{type_name} = 'reliable' Domain  ⨝ webpage ⨝ article  ⨝ typ \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#3.2\n",
    "#gets two highest count authors\n",
    "σ rownum() > 0 and rownum() ≤ 2  τ_{F_count(author) desc} F_count(author) π_{author_name} σ_{type_name = 'fake'} author  ⨝ written_by  ⨝ article  ⨝ typ)\n",
    "\n",
    "#3.2 gets the highest by max function aggregate function\n",
    "F_max(F_count(author)) π_{author_name}  (\n",
    "    #This gets the authors by count in desc order\n",
    "    τ_{F_count(author) desc} F_count(author) π_{author_name} σ_{type_name = 'fake'} author  ⨝ written_by  ⨝ article  ⨝ typ)\n",
    "\n",
    "\n",
    "#3.2 Could create a view\n",
    "With YourTable as (SELECT       author_name,\n",
    "             COUNT(author) AS value_occurrence \n",
    "    FROM     author NATURAL JOIN written_by NATURAL JOIN article NATURAL JOIN typ\n",
    "    WHERE    type_name = 'fake'\n",
    "    GROUP BY author_name)\n",
    "\n",
    "SELECT author_name\n",
    "FROM YourTable\n",
    "WHERE value_occurrence = (SELECT MAX(value_occurrence)\n",
    "                            FROM YourTable);\n",
    "\n",
    "#author_name preciding is group by\n",
    "π_{author_name} \n",
    "    author_name F_count(author) π_{author_name} σ_{type_name = 'fake'} author  ⨝ written_by  ⨝ article  ⨝ typ)   \n",
    "    σ_{F_count(author)} = (π_{MAX(F_count(author))} \n",
    "                         (author_name F_count(author) π_{author_name} σ_{type_name = 'fake'} author  ⨝ written_by  ⨝ article  ⨝ typ)    \n",
    "                           \n",
    "                           \n",
    "\"\"\"\n",
    "WITH small_tags AS (SELECT * FROM tag WHERE article_id <= 1000), articles_small AS (SELECT DISTINCT article_id FROM small_tags)\n",
    "\n",
    " \n",
    "\n",
    "SELECT A1.article_id, A2.article_id\n",
    "\n",
    " \n",
    "\n",
    "FROM articles_small A1, articles_small A2\n",
    "\n",
    " \n",
    "\n",
    "WHERE A1.article_id > A2.article_id\n",
    "AND EXISTS (SELECT *\n",
    "FROM small_tags\n",
    "WHERE A1.article_id = small_tags.article_id\n",
    ")\n",
    "\n",
    " \n",
    "\n",
    "AND NOT EXISTS ((SELECT small_tags.keyword_id\n",
    "from small_tags\n",
    "WHERE A1.article_id = small_tags.article_id ) EXCEPT (SELECT small_tags.keyword_id\n",
    "FROM small_tags\n",
    "WHERE A2.article_id = small_tags.article_id ) \n",
    ")\n",
    "\n",
    " \n",
    "\n",
    "AND NOT EXISTS ((SELECT small_tags.keyword_id\n",
    "from small_tags\n",
    "WHERE A2.article_id = small_tags.article_id ) EXCEPT (SELECT small_tags.keyword_id\n",
    "FROM small_tags\n",
    "WHERE A1.article_id = small_tags.article_id ) \n",
    ")\"\"\"                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 - Explore the FakeNewsCorpus dataset\n",
    "\n",
    "Make at least three non-trivial observations/discoveries about the data. These observations could be related to outliers, artefacts, or even better: genuinely interesting patterns in the data that could potentially be used for fake-news detection. Examples of simple observations could be how many missing values there are in particular columns - or what the distribution over domains is. Be creative! :). Note that many of these observations can be extracted as direct queries to the database, for instance using GROUP BY and COUNT."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CREATE MATERIALIZED VIEW\n",
    "articles_per_domain_and_type\n",
    "\t(domain, typ, article_count)\n",
    "\tAS (SELECT domain_url, type_name, COUNT(*)\n",
    "\t\tFROM article NATURAL JOIN typ NATURAL JOIN webpage NATURAL JOIN domain\n",
    "\t\tWHERE domain_url IS NOT NULL AND type_name != '<null>'\n",
    "\t\tGROUP BY domain_url, type_name);\n",
    "\n",
    "\n",
    "select * from articles_per_domain_and_type order by domain;\n",
    "\n",
    "\n",
    "CREATE MATERIALIZED VIEW\n",
    "articles_per_author_and_type\n",
    "\t(author, typ, article_count)\n",
    "\tAS (SELECT author_name, type_name, COUNT(*)\n",
    "\t\tFROM article NATURAL JOIN typ NATURAL JOIN written_by NATURAL JOIN author\n",
    "\t\tWHERE author_name != '<null>' AND type_name != '<null>'\n",
    "\t\tGROUP BY author_name, type_name);\n",
    "\n",
    "select * from articles_per_author_and_type order by author;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 - Creation of our own news data set by scraping it from the web.\n",
    "\n",
    "We will be looking at the \"Politics and Conflict\" section of the Wikinews site (https://en.wikinews.org/wiki/Category:Politics_and_conflicts), which contains about 7500 articles sorted by the first letter in their title. Since we want the different groups to have slightly different experiences with this data, each group should try to extract the articles for a specific range of letters - given by the python expression: \n",
    "\n",
    "\"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr%23:group_nr%23+10]\n",
    "\n",
    "where group_nr is your group number (according to Task 1). \n",
    "\n",
    "The data set you produce should contain fields corresponding to the content of the article, in addition to some metadata fields like the date when the article was written. Describe the tools you used, and any challenges that you faced, and report some basic statistics on the data (e.g. number of rows, fields, etc). Note that there are no fake/no-fake labels in this dataset - we will consider it as a trusted source of only true articles (which is perhaps a bit naive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testSpider(scrapy.Spider):\n",
    "    name = \"wikitest\"\n",
    "    def start_requests(self):\n",
    "        start_urls = [\n",
    "            \"https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from=D\",\n",
    "            '''\"https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from=E\",\n",
    "            \"https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from=F\",\n",
    "            \"https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from=G\",\n",
    "            \"https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from=H\",\n",
    "            \"https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from=I\",\n",
    "            \"https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from=J\",\n",
    "            \"https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from=K\",\n",
    "            \"https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from=L\",\n",
    "            \"https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from=M\",\n",
    "            \"https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from=N\",'''\n",
    "        ]\n",
    "\n",
    "\n",
    "        #yield all the article links from the start urls\n",
    "        for url in start_urls:\n",
    "            yield scrapy.Request( url = url, callback = self.parse )\n",
    "        \n",
    "        \n",
    "    #We want to follow the next page from the start urls until  we are finished entirely (at letter O)\n",
    "    #Will find all links in the start url that correspond to category\n",
    "    #Might also find some categories that should not be processed --- Fixed no longer applicable\n",
    "    def parse(self, response):    \n",
    "        links = response.xpath('/html/body/div[3]/div[3]/div[4]/div[2]/div[2]/div/div/div/ul/li/a/@href').extract()\n",
    "        WantedArticles = r\"/wiki/[D-N]\"\n",
    "        for link in links:\n",
    "            if link and (re.match(WantedArticles , link ) != None):\n",
    "                yield response.follow(url = link, callback = self.parse2)\n",
    "\n",
    "        #Change the last in the string to the last letter\n",
    "        forbiddenNP =  r\"https:\\/\\/en\\.wikinews\\.org\\/w\\/index\\.php\\?title=Category:Politics_and_conflicts&pagefrom=O\"\n",
    "        #This has to be get() instead of extract so that forbiddenNP in nextpageurl can be compared as strings\n",
    "        nextpageurl1 = response.xpath(\"//*[@id='mw-pages']/a[contains(.,'next page')]/@href\").get()\n",
    "        nextpageurl = response.urljoin(nextpageurl1)\n",
    "\n",
    "        #Obama signs $787 billion stimulus package #Last on the N page\n",
    "        #Obama signs healthcare bill for 9/11 emergency workers #first on the O page\n",
    "        if nextpageurl and (re.match(forbiddenNP , nextpageurl ) == None):\n",
    "            # If we've found a pattern which matches\n",
    "            #print(\"Found url: {}\".format(nextpageurl)) # Write a debug statement\n",
    "            yield scrapy.Request(nextpageurl, callback=self.parse) # Return a call to the function \"parse\"\n",
    "\n",
    "    #Will run over all individual articles and extract appropriate data\n",
    "    def parse2(self, response):\n",
    "        #return individual article \n",
    "        #title\n",
    "        title = response.xpath('/html/body/div[3]/h1/text()').get()\n",
    "        #source(s)\n",
    "        source = response.xpath('/html/body/div[3]/div[3]/div[4]/div/ul/li/span/descendant-or-self::text()').extract()\n",
    "        #date\n",
    "        date = response.xpath('/html/body/div[3]/div[3]/div[4]/div/p[1]/strong/text()').get()\n",
    "        #text\n",
    "        text = response.xpath(\"\"\"/html/body/div[3]/div[3]/div[4]/div/p/descendant-or-self::text()[not( parent::strong | ancestor::i)]\"\"\").extract()\n",
    "\n",
    "        finaltext = ''.join(text)\n",
    "\n",
    "        finalsource = ''.join(source)\n",
    "\n",
    "        yield {\n",
    "            'title': title,\n",
    "            'source' : finalsource,\n",
    "            'date' : date,\n",
    "            'text' : finaltext,\n",
    "        }\n",
    "\n",
    "        #put date text and source in right side of equal sign\n",
    "        #title as key\n",
    "        test_dict[title] = [finalsource , date , finaltext]\n",
    "\n",
    "#dictionary to store data\n",
    "test_dict = dict()\n",
    "\n",
    "#run spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(testSpider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of dict\n",
    "var = test_dict.keys()\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Difficulties in constructing the spider:\n",
    "Started with the approach of using all the start urls on the form of.\n",
    "\"https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from=<letter>\"\n",
    "    Where letter is [D-N]. Downside of this was that not all articles was on their start letter page. So some F articles was on another page and was thus not included. Furthermore we got some articles from other letters we solving this by checking for WantedArticles = r\"/wiki/[D-N]\".\n",
    "\n",
    "The next approach was to yield the parse() on itself when going to next page and starting on our D page. The spider would thus crawl until it reached a nextpage with a url with the letter O (The letter after N). \n",
    "\n",
    "There was generally a difficulty with parsing on multiple levels. But visualizing it as a spider crawling through a net helped on this part. So the spider would start on a node in the web corresponding to our start_urls. This node would then be connected to all the articles' nodes in the start_urls. All these article nodes would then be connected to strings yielding all the subparts of the articles that we would want to extract. When done extracting the spider would crawl back through the web to the next article and repeat until all articles was done. After it would then crawl to the nextpage, given the conditions to be true, and repeat until it could no longer crawl to a next page.\n",
    "\n",
    "If you would want to extract articles in range over the start of the alphabeth, eg. [S-Z,A-B], you would want two starturls on S and A. And as a restriction on the nextpage you would have C. Reasoning that no restriction is required after Z is that no nextpage exists on Z and the condition would therefore be False.\n",
    "\n",
    "Extracting the specific information from each article was a bit of a hassle. Some articles' content are structered in tables while over 90% have their content in paragraphs. We did not end up extracting all the content from these types of articles. Likewise with the source, sometimes it we were unable to destinquish this from the content as they were in the same structure.\n",
    "\n",
    "We have 29824 lines of data. We have 2747 articles, approximating to about 1/3 of the total articles, with four fields each. These are title, date, source, and text (content). Some articles have some empty fields due to the reasons descripted above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
